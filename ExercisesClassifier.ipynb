{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a2037-e483-4a6a-868a-202fb20791e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.utils import to_categorical , plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7a49c-a686-4d22-aba4-25d6d12594f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "#for drawing\n",
    "mp_drawing = mp.solutions.drawing_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd9ad4-303c-49ef-81a0-78cabad37b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "# Specify the path to the zip file\n",
    "zip_file_path = 'Dataset.zip'\n",
    "\n",
    "# Specify the directory where you want to extract the files\n",
    "extracted_dir = 'extracted_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee8bd7-28f2-403d-9bff-b03fdea09aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(extracted_dir, exist_ok=True)\n",
    "\n",
    "# Extract the files from the zip archive\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_dir)\n",
    "\n",
    "# List the extracted files\n",
    "extracted_files = os.listdir(extracted_dir)\n",
    "\n",
    "# Process each extracted file as needed\n",
    "for file_name in extracted_files:\n",
    "    file_path = os.path.join(extracted_dir, file_name)\n",
    "    # Do something with the file (e.g., read, process)\n",
    "    print(f\"Processing file: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf41557-5dab-4d51-9634-c564d4ca9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing the exercise folders\n",
    "ucf50_dir = os.path.join(extracted_dir,'Dataset')\n",
    "\n",
    "# List the exercise folders in the UCF50 directory\n",
    "exercises = os.listdir(ucf50_dir)\n",
    "\n",
    "# Print the names of the exercise folders\n",
    "for exercise_folder in exercises:\n",
    "    print(exercise_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa52dcd-1b7c-43f1-bc56-1c467e9ce5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'extracted_data/Dataset/'\n",
    "ACTIONS = np.array(['JumpingJack','PullUps','PushUps','Squats','Situp'])\n",
    "SEQUENCE_LENGTH = 30 # 30 Frames\n",
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3221367-089c-46b8-b64d-01e588292b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_range = random.sample(range(len(exercises)),5)\n",
    "plt.figure(figsize=(50,50))\n",
    "\n",
    "for counter,random_index in enumerate(random_range,1):\n",
    "    selected_class_name = exercises[random_index]\n",
    "    video_names = os.listdir(os.path.join(DATA_PATH,selected_class_name))\n",
    "    video_file = random.choice(video_names)\n",
    "    video_reader = cv2.VideoCapture(os.path.join(DATA_PATH,selected_class_name,video_file))\n",
    "    _,bgr_frame  = video_reader.read()\n",
    "\n",
    "    video_reader.release()\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(bgr_frame,cv2.COLOR_BGR2RGB)\n",
    "    cv2.putText(rgb_frame,selected_class_name,(10,30),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2)\n",
    "    plt.subplot(5,4,counter)\n",
    "    plt.imshow(rgb_frame)\n",
    "    plt.axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa35d8c-f16a-4a75-8041-27d1b18d29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_class = 100000000000000\n",
    "for cls in ACTIONS:\n",
    "    minimum_class = min(minimum_class,len(os.listdir(os.path.join(DATA_PATH,cls))))\n",
    "    print(f'class name : {cls} | {len(os.listdir(os.path.join(DATA_PATH,cls)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77678b7-e96a-4675-859d-bb8f2d53f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only we takes pose to make it Dense as much as possiable\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return np.array(pose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7ed72-e0c4-43b2-b631-8071e3b73bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image) #make prediction\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    return image,results   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1845bf-bcf0-43a7-b979-0c6548310f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only we takes pose to make it Dense as much as possiable\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return np.array(pose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a2aa2-0e55-41af-b59a-397e57b96634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image) #make prediction\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    return image,results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8de20-2bf6-4d1e-9b99-bb643c0507fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image,results):\n",
    "    mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_holistic.POSE_CONNECTIONS)\n",
    "    #mp_drawing.draw_landmarks(image,results.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS)\n",
    "    #mp_drawing.draw_landmarks(image,results.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fdcd4a-f799-4d67-a752-59294fa88768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "    keypoints_list = []\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "    video_frames_num = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))#property\n",
    "    skip_frames_window = max(int(video_frames_num/SEQUENCE_LENGTH),1)\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "        for frame_counter in range(SEQUENCE_LENGTH):\n",
    "            video_reader.set(cv2.CAP_PROP_POS_FRAMES,frame_counter*skip_frames_window)\n",
    "\n",
    "            ok,frame = video_reader.read()\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            image , result = mediapipe_detection(image=frame,model=holistic)\n",
    "            keypoints = extract_keypoints(result)\n",
    "\n",
    "\n",
    "            keypoints_list.append(keypoints)\n",
    "\n",
    "        video_reader.release()\n",
    "        return keypoints_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f34d48-6afd-4beb-bc90-62dde8c60939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_dataset(data_path,Classes_List):\n",
    "    features = []\n",
    "    labels = []\n",
    "    video_paths = []\n",
    "    for class_index,class_name in enumerate(Classes_List):\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "        files_list = os.listdir(os.path.join(data_path,class_name))\n",
    "        for filename in files_list:\n",
    "            video_path = os.path.join(data_path,class_name,filename)\n",
    "            frames = frames_extraction(video_path=video_path)\n",
    "            if len(frames) == SEQUENCE_LENGTH:\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "                video_paths.append(video_path)\n",
    "\n",
    "    return features, labels, video_paths  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e2dad-6a3c-4335-985e-d9ce0fa0db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features,labels,video_paths = Create_dataset(data_path=DATA_PATH,Classes_List=ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bec33c-244d-4425-81f5-ab340e4b9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997bf0eb-aee4-484d-b4a3-9129ca264f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59cf6fe6-7054-45be-ba26-55820a09ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features) #no. of videos , seq_len,number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f218b-be9d-4fa9-b33d-49641d84fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947ec6e-234f-4eae-97ad-986c7a705122",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features.pkl', 'wb') as file:\n",
    "    pickle.dump(features, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f0aad-e6df-47c5-94ce-e5d149631199",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features.pkl', 'rb') as file:\n",
    "    features = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e56dfe-bb98-49d1-b3e9-c76d1e048d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels.pkl', 'wb') as file:\n",
    "    pickle.dump(labels, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb48449-0245-4150-9b9a-6a9dcd15bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels.pkl', 'rb') as file:\n",
    "    labels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f943792b-2e74-4f95-aa72-25279ff164f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658eecb1-dadb-4fd2-b558-fd415210198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67bad73a-7f3b-4136-b062-42555d12cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test\n",
    "features_train , features_test , labels_train , labels_test = train_test_split(\n",
    "    features,\n",
    "    one_hot,\n",
    "    test_size=0.15,\n",
    "    shuffle=True,\n",
    "    random_state=27\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207cc8b6-3c3b-4683-af4d-876255543f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train features shape : {features_train.shape} | labels train shape : {labels_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88ee3f-b7d5-40fa-802b-4fdb48626714",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'test features shape : {features_test.shape} | labels test shape : {labels_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95119d10-a7cb-47a1-9c4d-27b49766686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bcaec4-b53e-4359-ba10-69a5c0559744",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5881c-229c-45c5-8e7f-06d5ec32d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ff151-9f33-419d-aaa4-d36cd3a34e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42b3e6b8-cb76-4d65-bd8c-3dbe13a6c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class ConfusionMatrixCallback(Callback):\n",
    "    def __init__(self, model, features, labels):\n",
    "        super(ConfusionMatrixCallback, self).__init__()\n",
    "        self.model = model\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        predictions = np.argmax(self.model.predict(self.features), axis=1)\n",
    "        true_labels = np.argmax(self.labels, axis=1)\n",
    "        cm = confusion_matrix(true_labels, predictions)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce165aba-6f50-40d8-9f22-c11c35edac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import keras_tuner\n",
    "\n",
    "def create_LSTM_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, activation=hp.Choice('activation', ['relu', 'tanh']), input_shape=(30, 132)))\n",
    "    model.add(LSTM(128, return_sequences=True, activation=hp.Choice('activation', ['relu', 'tanh'])))\n",
    "    model.add(LSTM(64, return_sequences=False, activation=hp.Choice('activation', ['relu', 'tanh'])))\n",
    "    model.add(Dense(hp.Choice('units', [16, 32, 64]), activation='relu'))\n",
    "    model.add(Dense(hp.Choice('units', [8, 16, 32]), activation='relu'))\n",
    "    model.add(Dense(ACTIONS.shape[0], activation='softmax'))\n",
    "    \n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\", default=1e-3)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05537696-85dd-486a-9549-918109488571",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    create_LSTM_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ed1c2-737d-406f-8a75-805b0bcfeb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(features_train, labels_train, epochs=5,validation_split=0.20,verbose=1)\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f2da1f3-d47b-4d19-bae7-fee775e398a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "17/17 [==============================] - 12s 204ms/step - loss: 0.1532 - accuracy: 0.9542 - val_loss: 0.4057 - val_accuracy: 0.8939\n",
      "Epoch 2/5\n",
      "17/17 [==============================] - 2s 90ms/step - loss: 0.1350 - accuracy: 0.9580 - val_loss: 0.3235 - val_accuracy: 0.9015\n",
      "Epoch 3/5\n",
      "17/17 [==============================] - 2s 90ms/step - loss: 0.1791 - accuracy: 0.9466 - val_loss: 0.2816 - val_accuracy: 0.9167\n",
      "Epoch 4/5\n",
      "17/17 [==============================] - 1s 88ms/step - loss: 0.1609 - accuracy: 0.9485 - val_loss: 0.4143 - val_accuracy: 0.8864\n",
      "Epoch 5/5\n",
      "17/17 [==============================] - 1s 83ms/step - loss: 0.1212 - accuracy: 0.9676 - val_loss: 0.3721 - val_accuracy: 0.8864\n"
     ]
    }
   ],
   "source": [
    "history = best_model.fit(features_train, labels_train, epochs=5, validation_split=0.20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fee087-e4a3-4cb6-a3de-9a7a78bcff75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b81d4-bd9d-4b21-a4ad-2949ec07e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0f4f4-ed8c-4c08-be09-4f4396a9ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate(features_test,labels_test,callbacks=[confusion_matrix_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de64d6f-7b1a-4aa5-815d-8908a8af813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already trained your 'best_model' and obtained evaluation results\n",
    "# Replace these with your actual evaluation results\n",
    "evaluation_results = best_model.evaluate(features_test, labels_test)\n",
    "loss = evaluation_results[0]  # Assuming loss is the first metric\n",
    "accuracy = evaluation_results[1]  # Assuming accuracy is the second metric\n",
    "\n",
    "# Define the metrics and their values for plotting\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "loss_values = history.history['loss']\n",
    "val_loss_values = history.history['val_loss']\n",
    "accuracy_values = history.history['accuracy']\n",
    "val_accuracy_values = history.history['val_accuracy']\n",
    "\n",
    "# Plotting the evaluation results\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss_values, label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, accuracy_values, label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy_values, label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f92c8-98a3-4c84-88e8-952abd28681b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "best_model = load_model('BestModel.keras')\n",
    "\n",
    "# Evaluate the model on test data\n",
    "evaluation_results = best_model.evaluate(features_test, labels_test)\n",
    "loss = evaluation_results[0]  # Assuming loss is the first metric\n",
    "accuracy = evaluation_results[1]  # Assuming accuracy is the second metric\n",
    "\n",
    "# Plotting the evaluation results\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot loss and accuracy on the same figure\n",
    "plt.bar(['Loss'], [loss], color='blue', label='Loss')\n",
    "plt.bar(['Accuracy'], [accuracy], color='green', label='Accuracy')\n",
    "\n",
    "plt.title('Evaluation Results')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ada0ca-578d-4b70-80d7-b9fc1c603f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b776f-ae73-4510-9690-d8c5052d14c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('BestModel.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48511131-c992-41a1-b07b-4bc8076d4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_webcam(model, sequence_length=30):\n",
    "    # Initialize the VideoCapture object to read from the webcam (assuming it's the first webcam, change the index if needed).\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "    # Initialize a variable to store the predicted action being performed in real-time.\n",
    "    predicted_class_name = ''\n",
    "    frame_buffer = []\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        # Iterate until the video capture is active.\n",
    "        while True:\n",
    "            # Read the frame from the webcam.\n",
    "            ok, frame = video_capture.read()\n",
    "\n",
    "            # Check if frame is not read properly then break the loop.\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            # Pre-process the frame and extract keypoints.\n",
    "            _ , result = mediapipe_detection(image=frame, model=holistic)\n",
    "            keypoints = extract_keypoints(result)\n",
    "\n",
    "            if len(keypoints) > 0:  # Check if keypoints are detected\n",
    "                frame_buffer.append(keypoints)\n",
    "                if len(frame_buffer) == sequence_length:\n",
    "                    # Convert the frame buffer to a numpy array and feed it to the model.\n",
    "                    frame_sequence = np.array(frame_buffer)[np.newaxis, ...]\n",
    "                    predicted_labels_probabilities = model.predict(frame_sequence)[0]\n",
    "                    predicted_label = np.argmax(predicted_labels_probabilities)\n",
    "                    predicted_class_name = ACTIONS[predicted_label]\n",
    "                    frame_buffer.pop(0)  # Remove the oldest frame from the buffer\n",
    "\n",
    "            # Write predicted class name on top of the frame.\n",
    "            cv2.putText(frame, 'Result:', (0, 115), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, predicted_class_name, (int(frame.shape[1] / 4), 115), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "            # Display the frame.\n",
    "            cv2.imshow('Real-time Action Recognition', frame)\n",
    "\n",
    "            # Check for 'q' key press to quit.\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    # Release the video capture object and close all OpenCV windows.\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85f2de70-afa3-4b05-837f-455ed2a93ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "predict_on_webcam(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328fef90-0c2f-4dc7-aea6-ffebe8df6b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
